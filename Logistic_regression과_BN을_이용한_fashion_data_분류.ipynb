{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression과 BN을 이용한 fashion data 분류",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMubnDREj1lg4IkdrPa8VB6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yeRkQgGgj-z"
      },
      "source": [
        "## Batch Normalization 예제\r\n",
        "### Input tensor 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRW8KUP0gXr2",
        "outputId": "541a6803-1954-4a8f-8804-b4714b6c7df3"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "input = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=torch.float)\r\n",
        "input = input.view((-1,2))\r\n",
        "print('input: ', input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  tensor([[ 1.,  2.],\n",
            "        [ 3.,  4.],\n",
            "        [ 5.,  6.],\n",
            "        [ 7.,  8.],\n",
            "        [ 9., 10.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HliuqViRiwC-"
      },
      "source": [
        "### Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qorN6aZzimgd",
        "outputId": "b505fc7e-da13-4d70-8621-3ab90abf3ac9"
      },
      "source": [
        "m = nn.BatchNorm1d(num_features=2)    # batch normalization을 하는데 숫자는 2개가 들어간다.\r\n",
        "bn_input = m(input)\r\n",
        "print('BN : ', bn_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BN :  tensor([[-1.4142, -1.4142],\n",
            "        [-0.7071, -0.7071],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.7071,  0.7071],\n",
            "        [ 1.4142,  1.4142]], grad_fn=<NativeBatchNormBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4-jBZjzjM6K"
      },
      "source": [
        "## FashionMNIST: Logistic regression + batch normalization\r\n",
        "##### 0. 티셔츠/상의\r\n",
        "##### 1. 바지\r\n",
        "##### 2. 스웨터\r\n",
        "##### 3. 드레스\r\n",
        "##### 4. 코트\r\n",
        "##### 5. 샌들\r\n",
        "##### 6. 셔츠\r\n",
        "##### 7. 운동화\r\n",
        "##### 8. 가방\r\n",
        "##### 9. 부츠 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snM-QQFqjTdl"
      },
      "source": [
        "### Fashion MNIST 학습데이터 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5bEE2DDjDxl",
        "outputId": "8738ceff-d9fd-4897-abc8-20ae703d8bfd"
      },
      "source": [
        "from torchvision.datasets import FashionMNIST\r\n",
        "from torchvision import transforms\r\n",
        "\r\n",
        "fashion_train = FashionMNIST('./', train=True, download=True, transform=transforms.ToTensor())\r\n",
        "print('fashion_train type: ', type(fashion_train))\r\n",
        "print('fhasion_train length: ', len(fashion_train))\r\n",
        "print('')\r\n",
        "print('fashion_train[0] type: ', type(fashion_train[0]))\r\n",
        "print('fashion_train[0] length: ', len(fashion_train[0]))\r\n",
        "print('')\r\n",
        "print('fashion_train[0][0] type: ', type(fashion_train[0][0]))\r\n",
        "print('fashion_train[0][0] info: ', fashion_train[0][0])\r\n",
        "print('')\r\n",
        "print('fashion_train[0][1] type: ', type(fashion_train[0][1]))\r\n",
        "print('fashion_train[0][1] info: ', fashion_train[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fashion_train type:  torchvision.datasets.mnist.FashionMNIST\n",
            "fhasion_train length:  60000\n",
            "\n",
            "fashion_train[0] type:  <class 'tuple'>\n",
            "fashion_train[0] length:  2\n",
            "\n",
            "fashion_train[0][0] type:  <class 'torch.Tensor'>\n",
            "fashion_train[0][0] info:  tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
            "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
            "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
            "          0.0157, 0.0000, 0.0000, 0.0118],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
            "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0471, 0.0392, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
            "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
            "          0.3020, 0.5098, 0.2824, 0.0588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
            "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
            "          0.5529, 0.3451, 0.6745, 0.2588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
            "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
            "          0.4824, 0.7686, 0.8980, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
            "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
            "          0.8745, 0.9608, 0.6784, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
            "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
            "          0.8627, 0.9529, 0.7922, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
            "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
            "          0.8863, 0.7725, 0.8196, 0.2039],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
            "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
            "          0.9608, 0.4667, 0.6549, 0.2196],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
            "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
            "          0.8510, 0.8196, 0.3608, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
            "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
            "          0.8549, 1.0000, 0.3020, 0.0000],\n",
            "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
            "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
            "          0.8784, 0.9569, 0.6235, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
            "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
            "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
            "          0.9137, 0.9333, 0.8431, 0.0000],\n",
            "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
            "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
            "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
            "          0.8627, 0.9098, 0.9647, 0.0000],\n",
            "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
            "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
            "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
            "          0.8706, 0.8941, 0.8824, 0.0000],\n",
            "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
            "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
            "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
            "          0.8745, 0.8784, 0.8980, 0.1137],\n",
            "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
            "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
            "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
            "          0.8627, 0.8667, 0.9020, 0.2627],\n",
            "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
            "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
            "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
            "          0.7098, 0.8039, 0.8078, 0.4510],\n",
            "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
            "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
            "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
            "          0.6549, 0.6941, 0.8235, 0.3608],\n",
            "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
            "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
            "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
            "          0.7529, 0.8471, 0.6667, 0.0000],\n",
            "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
            "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
            "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
            "          0.3882, 0.2275, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
            "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
            "\n",
            "fashion_train[0][1] type:  <class 'int'>\n",
            "fashion_train[0][1] info:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvUqTcBij4Pq"
      },
      "source": [
        "### Fashion MNIST 테스트 데이터 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1TasuA6jnVo",
        "outputId": "b769069b-61d3-42e3-9785-49a983df1946"
      },
      "source": [
        "fashion_test = FashionMNIST('./', train=False, download=True, transform= transforms.ToTensor())\r\n",
        "print('fashion_train type: ', type(fashion_train))\r\n",
        "print('fhasion_train length: ', len(fashion_train))\r\n",
        "print('')\r\n",
        "print('fashion_train[0] type: ', type(fashion_train[0]))\r\n",
        "print('fashion_train[0] length: ', len(fashion_train[0]))\r\n",
        "print('')\r\n",
        "print('fashion_train[0][0] type: ', type(fashion_train[0][0]))\r\n",
        "print('fashion_train[0][0] info: ', fashion_train[0][0])\r\n",
        "print('')\r\n",
        "print('fashion_train[0][1] type: ', type(fashion_train[0][1]))\r\n",
        "print('fashion_train[0][1] info: ', fashion_train[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fashion_train type:  torchvision.datasets.mnist.FashionMNIST\n",
            "fhasion_train length:  60000\n",
            "\n",
            "fashion_train[0] type:  <class 'tuple'>\n",
            "fashion_train[0] length:  2\n",
            "\n",
            "fashion_train[0][0] type:  <class 'torch.Tensor'>\n",
            "fashion_train[0][0] info:  tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
            "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
            "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
            "          0.0157, 0.0000, 0.0000, 0.0118],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
            "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0471, 0.0392, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
            "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
            "          0.3020, 0.5098, 0.2824, 0.0588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
            "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
            "          0.5529, 0.3451, 0.6745, 0.2588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
            "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
            "          0.4824, 0.7686, 0.8980, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
            "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
            "          0.8745, 0.9608, 0.6784, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
            "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
            "          0.8627, 0.9529, 0.7922, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
            "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
            "          0.8863, 0.7725, 0.8196, 0.2039],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
            "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
            "          0.9608, 0.4667, 0.6549, 0.2196],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
            "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
            "          0.8510, 0.8196, 0.3608, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
            "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
            "          0.8549, 1.0000, 0.3020, 0.0000],\n",
            "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
            "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
            "          0.8784, 0.9569, 0.6235, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
            "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
            "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
            "          0.9137, 0.9333, 0.8431, 0.0000],\n",
            "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
            "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
            "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
            "          0.8627, 0.9098, 0.9647, 0.0000],\n",
            "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
            "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
            "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
            "          0.8706, 0.8941, 0.8824, 0.0000],\n",
            "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
            "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
            "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
            "          0.8745, 0.8784, 0.8980, 0.1137],\n",
            "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
            "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
            "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
            "          0.8627, 0.8667, 0.9020, 0.2627],\n",
            "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
            "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
            "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
            "          0.7098, 0.8039, 0.8078, 0.4510],\n",
            "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
            "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
            "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
            "          0.6549, 0.6941, 0.8235, 0.3608],\n",
            "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
            "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
            "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
            "          0.7529, 0.8471, 0.6667, 0.0000],\n",
            "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
            "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
            "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
            "          0.3882, 0.2275, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
            "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
            "\n",
            "fashion_train[0][1] type:  <class 'int'>\n",
            "fashion_train[0][1] info:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awATqHeYkI1O"
      },
      "source": [
        "### Dataloader를 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAAmcQOhkFV9",
        "outputId": "17af64cc-72ed-4406-e5f4-12f6719b686a"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "import torch\r\n",
        "\r\n",
        "fashion_train_dataloader = DataLoader(fashion_train, batch_size=64, shuffle=True)\r\n",
        "fashion_test_dataloader = DataLoader(fashion_test, batch_size=20000, shuffle=True)\r\n",
        "\r\n",
        "for x1, y1 in fashion_train_dataloader:\r\n",
        "  x1 = x1.view(-1, 784)   # -1은 64 \r\n",
        "  print(x1.size(), x1.device, x1.dtype)\r\n",
        "  break;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784]) cpu torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8I9nMalkzgO"
      },
      "source": [
        "### 학습을 위한 모델, Loss, Opimizer 설정\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL2Svxt2k81a"
      },
      "source": [
        "### 다층 nn.Linear를 이용한 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr5aUrDuktPq"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "\r\n",
        "net = nn.Sequential(\r\n",
        "    nn.Linear(784, 392),\r\n",
        "    nn.BatchNorm1d(392),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(392, 196),\r\n",
        "    nn.BatchNorm1d(196),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(196, 98),\r\n",
        "    nn.BatchNorm1d(98),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(98, 49),\r\n",
        "    nn.BatchNorm1d(49),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(49, 24),\r\n",
        "    nn.BatchNorm1d(24),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(24, 10),\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YLQ-OqdlbCr"
      },
      "source": [
        "### Loss함수: Cross Entropy\r\n",
        "### Optimizer: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1ZN5DbPlY6t"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qTimqZ8lomU"
      },
      "source": [
        "### 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfFZRieQlnfT",
        "outputId": "99ed10c4-745b-4c09-90a4-33580b2bfc89"
      },
      "source": [
        "losses = []\r\n",
        "\r\n",
        "net.to('cuda')\r\n",
        "\r\n",
        "for epoc in range(200):\r\n",
        "\r\n",
        "  batch_loss = 0.0    # 64장 train을 할 때마다 나오는 loss값을 저장하기 위해\r\n",
        "  net.train()\r\n",
        "  for x_train, y_train in fashion_train_dataloader:\r\n",
        "\r\n",
        "    x_train = x_train.to(torch.device('cuda'))\r\n",
        "    y_train = y_train.to(torch.device('cuda'))\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    y_pred = net(x_train.view((-1, 784)))\r\n",
        "\r\n",
        "    loss = loss_fn(y_pred, y_train)\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "    batch_loss += loss.item()\r\n",
        "  losses.append(batch_loss)\r\n",
        "  print(epoc, 'Loss: ', batch_loss)\r\n",
        "\r\n",
        "  net.eval()\r\n",
        "  with torch.no_grad():\r\n",
        "\r\n",
        "    for x_test, y_test in fashion_test_dataloader:\r\n",
        "      x_test = x_test.to(torch.device('cuda'))\r\n",
        "      y_test = y_test.to(torch.device('cuda'))\r\n",
        "\r\n",
        "      test_result = net(x_test.view((-1, 784)))\r\n",
        "      pred = torch.argmax(test_result, dim=1)\r\n",
        "\r\n",
        "      num_correct = (pred == y_test).sum().item()\r\n",
        "      print('Accuracy: ', num_correct * 100.0 / len(y_test), '%')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Loss:  541.5810749828815\n",
            "Accuracy:  83.45 %\n",
            "1 Loss:  347.2350444793701\n",
            "Accuracy:  85.84 %\n",
            "2 Loss:  309.27279780060053\n",
            "Accuracy:  87.57 %\n",
            "3 Loss:  283.9241281673312\n",
            "Accuracy:  86.76 %\n",
            "4 Loss:  265.0354684740305\n",
            "Accuracy:  88.01 %\n",
            "5 Loss:  250.1825776696205\n",
            "Accuracy:  88.94 %\n",
            "6 Loss:  235.07106790691614\n",
            "Accuracy:  87.97 %\n",
            "7 Loss:  217.81223966926336\n",
            "Accuracy:  88.3 %\n",
            "8 Loss:  208.31549189984798\n",
            "Accuracy:  88.42 %\n",
            "9 Loss:  199.4767458513379\n",
            "Accuracy:  88.71 %\n",
            "10 Loss:  189.27533126622438\n",
            "Accuracy:  88.97 %\n",
            "11 Loss:  176.01162122935057\n",
            "Accuracy:  89.21 %\n",
            "12 Loss:  170.69565223902464\n",
            "Accuracy:  88.57 %\n",
            "13 Loss:  161.30584174767137\n",
            "Accuracy:  89.2 %\n",
            "14 Loss:  153.40643688850105\n",
            "Accuracy:  88.72 %\n",
            "15 Loss:  148.4685407858342\n",
            "Accuracy:  88.23 %\n",
            "16 Loss:  138.23066966235638\n",
            "Accuracy:  89.13 %\n",
            "17 Loss:  133.0728054214269\n",
            "Accuracy:  89.2 %\n",
            "18 Loss:  129.44950994476676\n",
            "Accuracy:  88.11 %\n",
            "19 Loss:  120.07893183082342\n",
            "Accuracy:  89.39 %\n",
            "20 Loss:  114.9478285368532\n",
            "Accuracy:  89.71 %\n",
            "21 Loss:  113.06458086520433\n",
            "Accuracy:  89.54 %\n",
            "22 Loss:  105.84005096554756\n",
            "Accuracy:  89.76 %\n",
            "23 Loss:  103.68492531962693\n",
            "Accuracy:  89.58 %\n",
            "24 Loss:  97.81248018238693\n",
            "Accuracy:  89.44 %\n",
            "25 Loss:  95.55666531156749\n",
            "Accuracy:  89.46 %\n",
            "26 Loss:  91.78266558423638\n",
            "Accuracy:  89.81 %\n",
            "27 Loss:  88.61893046461046\n",
            "Accuracy:  89.79 %\n",
            "28 Loss:  85.12197797279805\n",
            "Accuracy:  89.61 %\n",
            "29 Loss:  79.01323376130313\n",
            "Accuracy:  89.93 %\n",
            "30 Loss:  79.47934326715767\n",
            "Accuracy:  88.85 %\n",
            "31 Loss:  75.03913483442739\n",
            "Accuracy:  89.25 %\n",
            "32 Loss:  72.0303624169901\n",
            "Accuracy:  89.5 %\n",
            "33 Loss:  69.30773481051438\n",
            "Accuracy:  89.51 %\n",
            "34 Loss:  67.83221540041268\n",
            "Accuracy:  89.35 %\n",
            "35 Loss:  67.10743833007291\n",
            "Accuracy:  89.65 %\n",
            "36 Loss:  67.0276655452326\n",
            "Accuracy:  89.78 %\n",
            "37 Loss:  62.68838457763195\n",
            "Accuracy:  89.11 %\n",
            "38 Loss:  60.66429416136816\n",
            "Accuracy:  89.72 %\n",
            "39 Loss:  57.260317272972316\n",
            "Accuracy:  89.6 %\n",
            "40 Loss:  58.8783555587288\n",
            "Accuracy:  89.78 %\n",
            "41 Loss:  56.21574857737869\n",
            "Accuracy:  89.28 %\n",
            "42 Loss:  51.875584620051086\n",
            "Accuracy:  89.12 %\n",
            "43 Loss:  54.40867357957177\n",
            "Accuracy:  89.35 %\n",
            "44 Loss:  51.576851026387885\n",
            "Accuracy:  89.17 %\n",
            "45 Loss:  45.6947286024224\n",
            "Accuracy:  89.84 %\n",
            "46 Loss:  50.82896827161312\n",
            "Accuracy:  89.5 %\n",
            "47 Loss:  50.31947053986369\n",
            "Accuracy:  90.01 %\n",
            "48 Loss:  45.08617439807858\n",
            "Accuracy:  89.72 %\n",
            "49 Loss:  47.54316762438975\n",
            "Accuracy:  89.57 %\n",
            "50 Loss:  43.01743130548857\n",
            "Accuracy:  89.59 %\n",
            "51 Loss:  42.77791652723681\n",
            "Accuracy:  89.79 %\n",
            "52 Loss:  40.89072358282283\n",
            "Accuracy:  89.5 %\n",
            "53 Loss:  40.10023497394286\n",
            "Accuracy:  89.66 %\n",
            "54 Loss:  42.86193571891636\n",
            "Accuracy:  89.45 %\n",
            "55 Loss:  40.124958357424475\n",
            "Accuracy:  90.16 %\n",
            "56 Loss:  40.10336449928582\n",
            "Accuracy:  89.42 %\n",
            "57 Loss:  37.073319954099134\n",
            "Accuracy:  89.55 %\n",
            "58 Loss:  37.47866412624717\n",
            "Accuracy:  89.83 %\n",
            "59 Loss:  36.79356545681367\n",
            "Accuracy:  89.24 %\n",
            "60 Loss:  38.93923187989276\n",
            "Accuracy:  89.73 %\n",
            "61 Loss:  33.44244110881118\n",
            "Accuracy:  89.56 %\n",
            "62 Loss:  33.79757195699494\n",
            "Accuracy:  89.49 %\n",
            "63 Loss:  36.117879562516464\n",
            "Accuracy:  89.65 %\n",
            "64 Loss:  32.617191687080776\n",
            "Accuracy:  89.75 %\n",
            "65 Loss:  32.881964212312596\n",
            "Accuracy:  89.66 %\n",
            "66 Loss:  32.43764886457939\n",
            "Accuracy:  89.63 %\n",
            "67 Loss:  32.25282289105235\n",
            "Accuracy:  89.68 %\n",
            "68 Loss:  33.81485770927975\n",
            "Accuracy:  90.01 %\n",
            "69 Loss:  31.701021017914172\n",
            "Accuracy:  89.83 %\n",
            "70 Loss:  31.42558700661175\n",
            "Accuracy:  90.05 %\n",
            "71 Loss:  28.00975528731942\n",
            "Accuracy:  90.18 %\n",
            "72 Loss:  28.362745163787622\n",
            "Accuracy:  89.94 %\n",
            "73 Loss:  28.704929191429983\n",
            "Accuracy:  89.44 %\n",
            "74 Loss:  29.710010151262395\n",
            "Accuracy:  89.54 %\n",
            "75 Loss:  28.00942201464204\n",
            "Accuracy:  89.34 %\n",
            "76 Loss:  25.129036530677695\n",
            "Accuracy:  89.7 %\n",
            "77 Loss:  27.157488103868673\n",
            "Accuracy:  90.12 %\n",
            "78 Loss:  27.22508356353501\n",
            "Accuracy:  89.84 %\n",
            "79 Loss:  23.88941268605413\n",
            "Accuracy:  90.0 %\n",
            "80 Loss:  28.884761632536538\n",
            "Accuracy:  89.72 %\n",
            "81 Loss:  23.371913135488285\n",
            "Accuracy:  89.75 %\n",
            "82 Loss:  26.393637885688804\n",
            "Accuracy:  89.69 %\n",
            "83 Loss:  22.86512956387014\n",
            "Accuracy:  90.07 %\n",
            "84 Loss:  26.74554565208382\n",
            "Accuracy:  89.48 %\n",
            "85 Loss:  23.59097851306433\n",
            "Accuracy:  89.25 %\n",
            "86 Loss:  24.854982388438657\n",
            "Accuracy:  89.92 %\n",
            "87 Loss:  21.431369253710727\n",
            "Accuracy:  89.77 %\n",
            "88 Loss:  24.463212936359923\n",
            "Accuracy:  89.58 %\n",
            "89 Loss:  22.121851978561608\n",
            "Accuracy:  89.77 %\n",
            "90 Loss:  24.42179544744431\n",
            "Accuracy:  89.68 %\n",
            "91 Loss:  21.561044016387314\n",
            "Accuracy:  89.93 %\n",
            "92 Loss:  21.586342645445256\n",
            "Accuracy:  89.93 %\n",
            "93 Loss:  24.437935010762885\n",
            "Accuracy:  90.06 %\n",
            "94 Loss:  20.700335789530072\n",
            "Accuracy:  90.26 %\n",
            "95 Loss:  23.32197518166504\n",
            "Accuracy:  89.83 %\n",
            "96 Loss:  20.362326655602374\n",
            "Accuracy:  89.97 %\n",
            "97 Loss:  20.46820429788204\n",
            "Accuracy:  89.45 %\n",
            "98 Loss:  20.7708165000804\n",
            "Accuracy:  89.6 %\n",
            "99 Loss:  20.649026507133385\n",
            "Accuracy:  89.49 %\n",
            "100 Loss:  21.988845077430597\n",
            "Accuracy:  89.85 %\n",
            "101 Loss:  18.572079476405634\n",
            "Accuracy:  89.54 %\n",
            "102 Loss:  19.76166233688855\n",
            "Accuracy:  89.82 %\n",
            "103 Loss:  19.189804651774466\n",
            "Accuracy:  89.85 %\n",
            "104 Loss:  18.451777873597166\n",
            "Accuracy:  89.7 %\n",
            "105 Loss:  20.670705652519246\n",
            "Accuracy:  89.53 %\n",
            "106 Loss:  19.330097769052372\n",
            "Accuracy:  89.22 %\n",
            "107 Loss:  17.882065297904774\n",
            "Accuracy:  89.65 %\n",
            "108 Loss:  20.464397067691607\n",
            "Accuracy:  89.41 %\n",
            "109 Loss:  19.14560041727964\n",
            "Accuracy:  89.98 %\n",
            "110 Loss:  17.211418126316858\n",
            "Accuracy:  90.18 %\n",
            "111 Loss:  17.911910232222\n",
            "Accuracy:  89.64 %\n",
            "112 Loss:  17.019169517749106\n",
            "Accuracy:  89.72 %\n",
            "113 Loss:  17.700104852265213\n",
            "Accuracy:  90.05 %\n",
            "114 Loss:  18.42506943790795\n",
            "Accuracy:  89.86 %\n",
            "115 Loss:  17.762069121126842\n",
            "Accuracy:  90.14 %\n",
            "116 Loss:  13.204231445546611\n",
            "Accuracy:  90.2 %\n",
            "117 Loss:  18.48147447336669\n",
            "Accuracy:  90.12 %\n",
            "118 Loss:  18.569726555171655\n",
            "Accuracy:  90.13 %\n",
            "119 Loss:  14.349185849088826\n",
            "Accuracy:  90.04 %\n",
            "120 Loss:  17.893552110188466\n",
            "Accuracy:  90.15 %\n",
            "121 Loss:  16.8565687533046\n",
            "Accuracy:  90.09 %\n",
            "122 Loss:  14.496267281763721\n",
            "Accuracy:  89.91 %\n",
            "123 Loss:  16.911807581091125\n",
            "Accuracy:  89.96 %\n",
            "124 Loss:  16.165345213055843\n",
            "Accuracy:  89.55 %\n",
            "125 Loss:  17.455913140889606\n",
            "Accuracy:  89.91 %\n",
            "126 Loss:  16.29644440302218\n",
            "Accuracy:  89.95 %\n",
            "127 Loss:  14.589998626230226\n",
            "Accuracy:  89.66 %\n",
            "128 Loss:  15.874615147316945\n",
            "Accuracy:  89.55 %\n",
            "129 Loss:  16.50101955120772\n",
            "Accuracy:  89.61 %\n",
            "130 Loss:  14.954208601804567\n",
            "Accuracy:  89.85 %\n",
            "131 Loss:  14.982154965131485\n",
            "Accuracy:  89.82 %\n",
            "132 Loss:  15.965857279239572\n",
            "Accuracy:  90.03 %\n",
            "133 Loss:  14.799920299949008\n",
            "Accuracy:  89.72 %\n",
            "134 Loss:  13.559068396672956\n",
            "Accuracy:  90.08 %\n",
            "135 Loss:  15.508057784900302\n",
            "Accuracy:  89.89 %\n",
            "136 Loss:  13.352948504834785\n",
            "Accuracy:  90.1 %\n",
            "137 Loss:  12.708469498531485\n",
            "Accuracy:  89.47 %\n",
            "138 Loss:  13.851141175175144\n",
            "Accuracy:  89.8 %\n",
            "139 Loss:  16.502629110349517\n",
            "Accuracy:  90.08 %\n",
            "140 Loss:  12.87635032961407\n",
            "Accuracy:  89.72 %\n",
            "141 Loss:  13.646203280120972\n",
            "Accuracy:  90.17 %\n",
            "142 Loss:  14.584554012617446\n",
            "Accuracy:  90.0 %\n",
            "143 Loss:  12.60842740971566\n",
            "Accuracy:  89.87 %\n",
            "144 Loss:  13.731072932590905\n",
            "Accuracy:  90.12 %\n",
            "145 Loss:  13.23260687252332\n",
            "Accuracy:  90.17 %\n",
            "146 Loss:  12.526433538318088\n",
            "Accuracy:  90.01 %\n",
            "147 Loss:  13.271332043783332\n",
            "Accuracy:  89.82 %\n",
            "148 Loss:  14.535343835595995\n",
            "Accuracy:  90.31 %\n",
            "149 Loss:  12.785204870910093\n",
            "Accuracy:  89.65 %\n",
            "150 Loss:  11.541952248018788\n",
            "Accuracy:  90.09 %\n",
            "151 Loss:  14.571864263642055\n",
            "Accuracy:  90.07 %\n",
            "152 Loss:  11.610998525062314\n",
            "Accuracy:  90.24 %\n",
            "153 Loss:  14.646248964651022\n",
            "Accuracy:  89.82 %\n",
            "154 Loss:  10.96427154806588\n",
            "Accuracy:  89.46 %\n",
            "155 Loss:  13.661382885526109\n",
            "Accuracy:  90.35 %\n",
            "156 Loss:  12.757327791572607\n",
            "Accuracy:  90.08 %\n",
            "157 Loss:  11.457847543861135\n",
            "Accuracy:  90.07 %\n",
            "158 Loss:  13.694113103596464\n",
            "Accuracy:  89.52 %\n",
            "159 Loss:  13.513538924853492\n",
            "Accuracy:  90.11 %\n",
            "160 Loss:  9.943934274513595\n",
            "Accuracy:  90.06 %\n",
            "161 Loss:  12.080078964489076\n",
            "Accuracy:  90.43 %\n",
            "162 Loss:  11.065934352263866\n",
            "Accuracy:  89.98 %\n",
            "163 Loss:  12.854458119143601\n",
            "Accuracy:  89.91 %\n",
            "164 Loss:  11.853133051663463\n",
            "Accuracy:  90.17 %\n",
            "165 Loss:  11.650084708388022\n",
            "Accuracy:  89.84 %\n",
            "166 Loss:  10.182159276861057\n",
            "Accuracy:  89.83 %\n",
            "167 Loss:  13.600545853623771\n",
            "Accuracy:  89.79 %\n",
            "168 Loss:  11.822966885940332\n",
            "Accuracy:  89.65 %\n",
            "169 Loss:  10.923731033170043\n",
            "Accuracy:  89.77 %\n",
            "170 Loss:  10.62492117263173\n",
            "Accuracy:  89.81 %\n",
            "171 Loss:  11.05065751019356\n",
            "Accuracy:  89.62 %\n",
            "172 Loss:  12.471786792037165\n",
            "Accuracy:  89.69 %\n",
            "173 Loss:  10.488125356743694\n",
            "Accuracy:  89.96 %\n",
            "174 Loss:  13.454198202354746\n",
            "Accuracy:  89.83 %\n",
            "175 Loss:  10.549019405050785\n",
            "Accuracy:  90.13 %\n",
            "176 Loss:  11.191648777134105\n",
            "Accuracy:  90.12 %\n",
            "177 Loss:  9.56084084988106\n",
            "Accuracy:  89.93 %\n",
            "178 Loss:  10.761188691354619\n",
            "Accuracy:  89.87 %\n",
            "179 Loss:  11.682425257509749\n",
            "Accuracy:  90.04 %\n",
            "180 Loss:  10.842685329902451\n",
            "Accuracy:  89.98 %\n",
            "181 Loss:  11.127551359142672\n",
            "Accuracy:  89.79 %\n",
            "182 Loss:  11.489453150770714\n",
            "Accuracy:  89.98 %\n",
            "183 Loss:  11.18503866937317\n",
            "Accuracy:  90.08 %\n",
            "184 Loss:  10.70435872123926\n",
            "Accuracy:  90.11 %\n",
            "185 Loss:  9.204749581285796\n",
            "Accuracy:  90.05 %\n",
            "186 Loss:  11.222786597263621\n",
            "Accuracy:  89.99 %\n",
            "187 Loss:  10.062694671541976\n",
            "Accuracy:  89.9 %\n",
            "188 Loss:  9.839107073235937\n",
            "Accuracy:  89.88 %\n",
            "189 Loss:  10.16377896636186\n",
            "Accuracy:  89.88 %\n",
            "190 Loss:  11.325250133970258\n",
            "Accuracy:  90.19 %\n",
            "191 Loss:  9.859315158757454\n",
            "Accuracy:  89.98 %\n",
            "192 Loss:  9.529137237484974\n",
            "Accuracy:  90.06 %\n",
            "193 Loss:  10.356970328764874\n",
            "Accuracy:  89.88 %\n",
            "194 Loss:  9.160366516203794\n",
            "Accuracy:  90.1 %\n",
            "195 Loss:  12.383954563912994\n",
            "Accuracy:  90.14 %\n",
            "196 Loss:  7.468110446667197\n",
            "Accuracy:  90.29 %\n",
            "197 Loss:  10.492173239068507\n",
            "Accuracy:  89.99 %\n",
            "198 Loss:  9.613092677112945\n",
            "Accuracy:  90.39 %\n",
            "199 Loss:  11.4866934406964\n",
            "Accuracy:  90.18 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hmB-Sbrm_ZU"
      },
      "source": [
        "### 학습 과정 중의 loss visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ukbnVzhbmpzp",
        "outputId": "14ca68dd-68d7-45b1-9fc4-a2dbc6c20f40"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9aeaf547f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}