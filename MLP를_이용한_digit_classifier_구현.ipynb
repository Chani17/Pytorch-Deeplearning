{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP를 이용한 digit classifier 구현",
      "provenance": [],
      "authorship_tag": "ABX9TyPxSs5+VRFdrx+BsVKIY85q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4Tb0igiYwdO"
      },
      "source": [
        "### 손글씨 숫자 데이터 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMC_vQVoVywp"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from sklearn.datasets import load_digits\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mqy6Ed5Y1lL",
        "outputId": "a01dd1ca-e079-44b9-fd15-41be218b59e1"
      },
      "source": [
        "digits = load_digits()\r\n",
        "\r\n",
        "X = digits.data\r\n",
        "print(type(X))\r\n",
        "print(X.shape)\r\n",
        "\r\n",
        "y = digits.target\r\n",
        "print(type(y))\r\n",
        "print(y.shape)\r\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(1797, 64)\n",
            "<class 'numpy.ndarray'>\n",
            "(1797,)\n",
            "[0 1 2 ... 8 9 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoAir4tKY6HF"
      },
      "source": [
        "### 학습과 테스트 데이터 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RtsdKTAY4cW",
        "outputId": "eea88eda-2abf-465a-e9e0-f37047c720e9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\r\n",
        "\r\n",
        "print('Type: x_train, y_train, x_test, y_test')\r\n",
        "print(type(x_train), type(y_train), type(x_test), type(y_test))\r\n",
        "\r\n",
        "print('Type: x_train, y_train, x_test, y_test')\r\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type: x_train, y_train, x_test, y_test\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "Type: x_train, y_train, x_test, y_test\n",
            "(1617, 64) (1617,) (180, 64) (180,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BblJVk_Zpau"
      },
      "source": [
        "### Tensor로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHTOGcDrZncV"
      },
      "source": [
        "x_train = torch.tensor(x_train, dtype=torch.float).to('cuda')\r\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to('cuda')\r\n",
        "x_test = torch.tensor(x_test, dtype=torch.float).to('cuda')\r\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxMHQS5EaxCY"
      },
      "source": [
        "### Dataset을 만든다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRss8q-daz0T",
        "outputId": "73302a17-1a7c-46cf-c9d6-694364003193"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "\r\n",
        "train_dataset = TensorDataset(x_train, y_train)\r\n",
        "test_dataset = TensorDataset(x_test, y_test)\r\n",
        "\r\n",
        "print(train_dataset)\r\n",
        "print(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataset.TensorDataset object at 0x7fc20d30e4e0>\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7fc20d30e9b0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOm66T1xa2wt"
      },
      "source": [
        "### Dataloader를 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKM4ToLDa43X",
        "outputId": "e814aacd-9f23-4d23-dcf0-fd12933a5f6b"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\r\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=10000, shuffle=True)  # 한번에 시험보려고(있는 만큼) -> 그래서 batch_size를 10000으로 설정\r\n",
        "\r\n",
        "print(train_dataset)\r\n",
        "print(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataset.TensorDataset object at 0x7fc20d30e4e0>\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7fc20d30e9b0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t2eGlaqbESj"
      },
      "source": [
        "## 학습을 위한 모델, Loss, Optimizer 설정\r\n",
        "### 다층 nn.Linear를 이용한 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXBxSmWIbLIS"
      },
      "source": [
        "net = nn.Sequential(\r\n",
        "    nn.Linear(64,32),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(32,16),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(16,10)\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmdohHQ1bPFW"
      },
      "source": [
        "### Loss함수: Cross Entropy\r\n",
        "### Optimizer: Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0LBmkhKbREW"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rR1vvkMaG_L"
      },
      "source": [
        "### 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCeggMTJZ5Ps",
        "outputId": "63bc3740-236a-4ff1-de70-45a38c33f75f"
      },
      "source": [
        "losses = []\r\n",
        "net.train()\r\n",
        "net.to('cuda')\r\n",
        "\r\n",
        "for epoc in range(200):\r\n",
        "\r\n",
        "  batch_loss = 0.0\r\n",
        "\r\n",
        "  for x_train, y_train in train_dataloader:\r\n",
        "    \r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    y_pred = net(x_train)\r\n",
        "    loss = loss_fn(y_pred, y_train)\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "    batch_loss += loss.item()\r\n",
        "  losses.append(batch_loss)\r\n",
        "  print(epoc, 'Loss: ', batch_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Loss:  26.284514278173447\n",
            "1 Loss:  7.388579174876213\n",
            "2 Loss:  5.019735589623451\n",
            "3 Loss:  2.9164623245596886\n",
            "4 Loss:  3.117889951914549\n",
            "5 Loss:  2.1765050273388624\n",
            "6 Loss:  2.768467809073627\n",
            "7 Loss:  1.824886185117066\n",
            "8 Loss:  2.0366579084657133\n",
            "9 Loss:  1.2465459667146206\n",
            "10 Loss:  0.837791244732216\n",
            "11 Loss:  0.8046924266964197\n",
            "12 Loss:  1.2730707505252212\n",
            "13 Loss:  1.7789897653274238\n",
            "14 Loss:  0.8311959808925167\n",
            "15 Loss:  0.9567399441730231\n",
            "16 Loss:  1.4191745278658345\n",
            "17 Loss:  0.7370247544022277\n",
            "18 Loss:  0.7325326055288315\n",
            "19 Loss:  0.2861303298268467\n",
            "20 Loss:  0.42062671412713826\n",
            "21 Loss:  1.901837164972676\n",
            "22 Loss:  3.1922713555395603\n",
            "23 Loss:  1.612598791718483\n",
            "24 Loss:  1.1683847661479376\n",
            "25 Loss:  0.48300310553167947\n",
            "26 Loss:  0.3413378060795367\n",
            "27 Loss:  0.19860022031934932\n",
            "28 Loss:  0.10243293996609282\n",
            "29 Loss:  0.023662979274376994\n",
            "30 Loss:  0.023698920987953898\n",
            "31 Loss:  0.021359430382290157\n",
            "32 Loss:  0.010863246832741424\n",
            "33 Loss:  0.009608004198526032\n",
            "34 Loss:  0.008648623595945537\n",
            "35 Loss:  0.007592539357574424\n",
            "36 Loss:  0.0068803117028437555\n",
            "37 Loss:  0.006596699403189632\n",
            "38 Loss:  0.006253343251955812\n",
            "39 Loss:  0.00649636379966978\n",
            "40 Loss:  0.005786174919194309\n",
            "41 Loss:  0.005434833656181581\n",
            "42 Loss:  0.0052297985639597755\n",
            "43 Loss:  0.005499480154867342\n",
            "44 Loss:  0.004980598663678393\n",
            "45 Loss:  0.004687897759140469\n",
            "46 Loss:  0.004384941888929461\n",
            "47 Loss:  0.004145245302794365\n",
            "48 Loss:  0.004023413956929289\n",
            "49 Loss:  0.0038484701155994117\n",
            "50 Loss:  0.004059081264131237\n",
            "51 Loss:  0.0036206177919666516\n",
            "52 Loss:  0.003792121258811676\n",
            "53 Loss:  0.0034959650538439746\n",
            "54 Loss:  0.0033377330137227545\n",
            "55 Loss:  0.0031542227770842146\n",
            "56 Loss:  0.003078835010455805\n",
            "57 Loss:  0.0032963937665044796\n",
            "58 Loss:  0.002996847866597818\n",
            "59 Loss:  0.0028065551014151424\n",
            "60 Loss:  0.0028212953820911935\n",
            "61 Loss:  0.0026197506178959884\n",
            "62 Loss:  0.0026957339941873215\n",
            "63 Loss:  0.002550214774601045\n",
            "64 Loss:  0.0027455050621938426\n",
            "65 Loss:  0.002468173210218083\n",
            "66 Loss:  0.002321455602441347\n",
            "67 Loss:  0.0023358527814707486\n",
            "68 Loss:  0.0021713457008445403\n",
            "69 Loss:  0.0021404509993203646\n",
            "70 Loss:  0.0022103202099970076\n",
            "71 Loss:  0.002105112977005774\n",
            "72 Loss:  0.0020082226928934688\n",
            "73 Loss:  0.0019232187460147543\n",
            "74 Loss:  0.0019527475105860503\n",
            "75 Loss:  0.0018981434632223682\n",
            "76 Loss:  0.0018896144665632164\n",
            "77 Loss:  0.001802045053409529\n",
            "78 Loss:  0.0017147420130640967\n",
            "79 Loss:  0.0016940152045208379\n",
            "80 Loss:  0.0016246033928837278\n",
            "81 Loss:  0.0016354345789295621\n",
            "82 Loss:  0.0015645731232325488\n",
            "83 Loss:  0.0015609064994350774\n",
            "84 Loss:  0.0015555120280623669\n",
            "85 Loss:  0.0015062303682498168\n",
            "86 Loss:  0.0014486312111330335\n",
            "87 Loss:  0.0018707056908624509\n",
            "88 Loss:  0.0014545351568813203\n",
            "89 Loss:  0.0014600267177229398\n",
            "90 Loss:  0.0013934661883467925\n",
            "91 Loss:  0.001390474473737413\n",
            "92 Loss:  0.001337686917395331\n",
            "93 Loss:  0.0013458621015161043\n",
            "94 Loss:  0.0012491903999034548\n",
            "95 Loss:  0.0011794494021160062\n",
            "96 Loss:  0.001179515345938853\n",
            "97 Loss:  0.001138484301918652\n",
            "98 Loss:  0.0011476570452941814\n",
            "99 Loss:  0.0011256647876507486\n",
            "100 Loss:  0.0010624846208884264\n",
            "101 Loss:  0.0011740623640434933\n",
            "102 Loss:  0.0010526805508561665\n",
            "103 Loss:  0.0010645402849149832\n",
            "104 Loss:  0.0010036069415946258\n",
            "105 Loss:  0.0010343212952648173\n",
            "106 Loss:  0.001028460555971833\n",
            "107 Loss:  0.000969244049883855\n",
            "108 Loss:  0.0009248012147509144\n",
            "109 Loss:  0.0009062474046004354\n",
            "110 Loss:  0.000905655525457405\n",
            "111 Loss:  0.0008710012498909236\n",
            "112 Loss:  0.0008529366991751886\n",
            "113 Loss:  0.0008555471104045864\n",
            "114 Loss:  0.000908850641508252\n",
            "115 Loss:  0.0008412614261033013\n",
            "116 Loss:  0.0008091282729765226\n",
            "117 Loss:  0.0008027325479815772\n",
            "118 Loss:  0.0007907366984909459\n",
            "119 Loss:  0.0007814194341335678\n",
            "120 Loss:  0.0007374711708507675\n",
            "121 Loss:  0.0007773487382110034\n",
            "122 Loss:  0.0007112560920177202\n",
            "123 Loss:  0.0006967057738052063\n",
            "124 Loss:  0.0007051651741676324\n",
            "125 Loss:  0.0007080999680511013\n",
            "126 Loss:  0.0006713650809615501\n",
            "127 Loss:  0.0006719251214235555\n",
            "128 Loss:  0.0006541921811731299\n",
            "129 Loss:  0.0006277972818224953\n",
            "130 Loss:  0.0006376614578584849\n",
            "131 Loss:  0.0006135486489711184\n",
            "132 Loss:  0.0006907030283400672\n",
            "133 Loss:  0.0005912924650601781\n",
            "134 Loss:  0.0005815117549445858\n",
            "135 Loss:  0.0005956109212092997\n",
            "136 Loss:  0.0005808610699205019\n",
            "137 Loss:  0.0005484936555149034\n",
            "138 Loss:  0.0005590325972661958\n",
            "139 Loss:  0.0005385697536439693\n",
            "140 Loss:  0.0005119874122101464\n",
            "141 Loss:  0.0005196757957719456\n",
            "142 Loss:  0.0005179039344511693\n",
            "143 Loss:  0.000495076797051297\n",
            "144 Loss:  0.00047461426902373205\n",
            "145 Loss:  0.00048740534839453176\n",
            "146 Loss:  0.0004585134641388322\n",
            "147 Loss:  0.00044768496184133255\n",
            "148 Loss:  0.0004586758175264549\n",
            "149 Loss:  0.0004362156973911624\n",
            "150 Loss:  0.00043815055323648266\n",
            "151 Loss:  0.0004423788841450005\n",
            "152 Loss:  0.0004397493189571833\n",
            "153 Loss:  0.0004133814222768706\n",
            "154 Loss:  0.0004093585052942217\n",
            "155 Loss:  0.0003942885896321968\n",
            "156 Loss:  0.0003916527930414304\n",
            "157 Loss:  0.0003893476527991879\n",
            "158 Loss:  0.00036820200318743446\n",
            "159 Loss:  0.0003751752090010996\n",
            "160 Loss:  0.00036146217092891675\n",
            "161 Loss:  0.00037199425128164876\n",
            "162 Loss:  0.00035752826647694746\n",
            "163 Loss:  0.0003489712643158782\n",
            "164 Loss:  0.0003424721635383321\n",
            "165 Loss:  0.000327649154911569\n",
            "166 Loss:  0.000321129617105953\n",
            "167 Loss:  0.00033516215580675635\n",
            "168 Loss:  0.00032960375074253534\n",
            "169 Loss:  0.0003230940887988254\n",
            "170 Loss:  0.00031231935247433285\n",
            "171 Loss:  0.0003000885686788024\n",
            "172 Loss:  0.0002962758867397497\n",
            "173 Loss:  0.00029846478491890593\n",
            "174 Loss:  0.00028661107467087277\n",
            "175 Loss:  0.00029033222836005734\n",
            "176 Loss:  0.0002910659886765643\n",
            "177 Loss:  0.00028046044076290855\n",
            "178 Loss:  0.0002699892497730616\n",
            "179 Loss:  0.00028632371686398983\n",
            "180 Loss:  0.0002577386090933942\n",
            "181 Loss:  0.00026900614989244787\n",
            "182 Loss:  0.0002597834679818334\n",
            "183 Loss:  0.0002500475779925182\n",
            "184 Loss:  0.00026060909010539035\n",
            "185 Loss:  0.00023949900162278936\n",
            "186 Loss:  0.00023659339785808697\n",
            "187 Loss:  0.00022796925641088706\n",
            "188 Loss:  0.00023489014142796805\n",
            "189 Loss:  0.0002260476769606612\n",
            "190 Loss:  0.00022105970697339217\n",
            "191 Loss:  0.00021512659645850363\n",
            "192 Loss:  0.00022404298420042323\n",
            "193 Loss:  0.00021026403692303575\n",
            "194 Loss:  0.00022690473466013827\n",
            "195 Loss:  0.0002095638824073376\n",
            "196 Loss:  0.00021372963510657428\n",
            "197 Loss:  0.0002079914869455024\n",
            "198 Loss:  0.00020238478759893042\n",
            "199 Loss:  0.00019128281161329141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naKGi-eKaOPx"
      },
      "source": [
        "### 학습과정 중의 loss visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "kPIc7XXMaIst",
        "outputId": "ac375a41-6fb7-4c2e-c720-b746e5b9c500"
      },
      "source": [
        "plt.plot(losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc1b0074940>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atpmlTzHcOwh"
      },
      "source": [
        "### 테스트 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09StdWutaSRa",
        "outputId": "2b70be50-5f34-491c-b147-2dafeee2bc2e"
      },
      "source": [
        "net.eval()\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  for x_test, y_test in test_dataloader:\r\n",
        "    test_result = net(x_test)\r\n",
        "    pred = torch.argmax(test_result, dim=1)\r\n",
        "\r\n",
        "    num_correct = (pred == y_test).sum().item()\r\n",
        "    print('Accuracy: ', num_correct * 100.0/len(y_test), '%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  98.33333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}